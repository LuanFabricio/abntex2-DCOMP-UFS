%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Lauro Cesar Araujo at 2015-04-27 19:43:45 -0300


%% Saved with string encoding Unicode (UTF-8)

@article{rafael,
	title = {A Resource Constrained Pipeline Approach to Embed Convolutional Neural Models (CNNs)},
	author = {Rafael Andrade da Silva},
	year = {2022},
	url = {https://drive.google.com/file/d/1yl0EMe8q0iasmqoZpCXEV9L5UO6bgfF4/view},
}

@article{leandro,
	title = {Edge FR - Desenvolvimento de um modelo de reconhecimento facial para dispositivos embarcados de baixo custo},
	author = {Leandro de Jesus Dias Santana},
	year = {2024},
}

% WARNING: Não achei o autor
@online{tf2guia,
	author = {TensorFlow},
	title = {Data augmentation  |  TensorFlow Core},
	url = {https://www.tensorflow.org/tutorials/images/data_augmentation},
	year = {2024},
	accessdate = {25-01-2024}
}

@article{frazier2018tutorial,
	title={A tutorial on Bayesian optimization},
	author={Frazier, Peter I},
	journal={arXiv preprint arXiv:1807.02811},
	year={2018}
}

@article{shorten2019survey,
	title={A survey on image data augmentation for deep learning},
	author={Shorten, Connor and Khoshgoftaar, Taghi M},
	journal={Journal of big data},
	volume={6},
	number={1},
	pages={1-48},
	year={2019},
	publisher={Springer}
}

@article{LIANG2021370,
	title = {Pruning and quantization for deep neural network acceleration: A survey},
	journal = {Neurocomputing},
	volume = {461},
	pages = {370-403},
	year = {2021},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2021.07.045},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
	author = {Tailin Liang and John Glossner and Lei Wang and Shaobo Shi and Xiaotong Zhang},
	keywords = {Convolutional neural network, Neural network acceleration, Neural network quantization, Neural network pruning, Low-bit mathematics},
	abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.}
}

@book{ml-faceli,
	title   = {Inteligência artificial: uma abordagem de aprendizado de máquina},
	author = {Faceli, Katti and Lorena, Ana Carolina and Gama, João and
	   Carvalho, André Carlos Ponce de Leon Ferreira de},
	year = {2011},
	publisher = {LTC}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{zhang2023dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    publisher={Cambridge University Press},
    note={\url{https://D2L.ai}},
    year={2023}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{lenet,
  title={UA M uller, E. S ackinger, P. Simard, and V. Vapnik. Comparison of learning algorithms for handwritten digit recognition},
  author={LeCun, Yann and Jackel, LD and Bottou, Leon and Brunot, A and Cortes, Corinna and Denker, JS and Drucker, Harris and Guyon, I},
  booktitle={Proceedings ICANN},
  volume={95},
  pages={53--60},
  year={1995}
}

@INPROCEEDINGS{resnet,
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	title={Deep Residual Learning for Image Recognition},
	year={2016},
	volume={},
	number={},
	pages={770-778},
	keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
	doi={10.1109/CVPR.2016.90}
}

@inproceedings{alexnet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	volume = {25},
	year = {2012}
}
