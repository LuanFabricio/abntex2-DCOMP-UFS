\chapter{Conceitos básicos}\label{cap_conceitos}

\chapterprecis{Isto é uma sinopse de capítulo. A ABNT não traz nenhuma
normatização a respeito desse tipo de resumo, que é mais comum em romances
e livros técnicos.}\index{sinopse de capítulo}

\section{Redes Neurais Artificiais}\label{cap_conceitos_ann}
% ---
Redes Neurais Artificiais (ANNs), são neurônios interconectados que realizam um processamento simples. Dentro dessa
estrutura cada neurônio reforça ou enfraquece a conexão com um dos neurônio da coluna anterior, assim replicando
o processo de aprendizagem do cérebro humano. A \autoref{cap_conceitos_ann_exemplo_ann} exemplifica uma ANN bem
simples.

\begin{figure}[htb]
	\caption {\label{cap_conceitos_ann_exemplo_ann}Exemplo de uma ANN}
	\begin{center}
		\includegraphics[scale=0.5]{Imagens/exemplo_nn}
	\end{center}
	\legend {Fonte: Autor}
\end{figure}

O neurônio é uma parte fundamental de uma ANN, nele que o aprendizado é armazenado através do reforço de conexões com
outros neurônio. Esse reforço é o peso da conexão, ele é multiplicado pela entrada e somado com os outros valores,
como é demonstrado na equação \ref{eq_neuronio} (onde $x$ é um vetor com os valores de entrada do neurônio e $w$
é um vetor com os pesos de cada entrada). Depois disso os valores passam por uma função de ativação $g(x)$ (\ref
{eq_ativacao}), que é responsável por "tratar" esses dados de saída antes que eles sejam passados para próxima etapa.

\begin{equation}\label{eq_neuronio}
u = \sum x_i w_i
\end{equation}
\begin{equation}\label{eq_ativacao}
y = g(u + b)
\end{equation}

\begin{figure}[htb]
	\caption {\label{cap_conceitos_ex_neuronio} Exemplo de um neurônio artificial}
	\begin{center}
		\includegraphics[scale=0.3]{Imagens/exemplo_neuronio_artificial}
	\end{center}
	\legend{Fonte: \cite{ml-faceli}}
\end{figure}

\section{Redes Neurais Convolucionais}\label{cap_conceitos_cnn}
% ---
Redes Neurais Convolucionais (CNNs) são Redes Neurais Artificiais (ANN)
que utilizam a operação de convolução para o processamento e análise de dados no formato de \textit{grid}(grade).
Por exemplo, uma série temporal que pode ser representada no formato de \textit{grid} 1-D,
ou uma imagem, que pode ser representada no formato 2-D. \cite{Goodfellow-et-al-2016}
Onde, LeNet \cite{lenet}, Residual Network (ResNet) \cite{resnet} e AlexNet \cite{alexnet} são alguns exemplos de CNNs
famosas.
% (TODO: Adicionar exemplos)

A arquitetura de uma CNN é composta por camadas convolucionais (\ref{cap_conceitos_cnn_conv}),
\textit{pooling} (\ref{cap_conceitos_cnn_pooling}) e totalmente conectadas (\ref{cap_conceitos_cnn_totalmente}),
como podemos ver na \autoref{exemplo_lenet}.

\begin{figure}[htb]
	\caption {\label{exemplo_lenet} Arquitetura da LeNet}
	\begin{center}
		\includegraphics[scale=0.5]{Imagens/lenet}
	\end{center}
	\legend{Fonte: \cite{zhang2023dive}}
\end{figure}

\subsection{Camada de Convolução}\label{cap_conceitos_cnn_conv}
Nessa camada são aplicados filtros (matriz de pesos) nos dados de entrada,
onde esses filtros deslizam cada célula da imagem executando operações de multiplicação e soma
em cada elemento da matriz de entrada, com o objetivo de gerar um mapa de características (\textit{feature map}).
O objetivo desses filtros é realçar as características dos dados de entrada, como curvas, linhas e outros padrões.
% (TODO: Adicionar figuras, referências e detalhar mais)

\subsection{Camada de \textit{pooling}}\label{cap_conceitos_cnn_pooling}
% ---
A abordagem da camada de \textit{pooling} é um pouco parecida com a camada de convolução,
uma matriz desliza pelas células da imagem, salvando apenas o maior valor dessa área na matriz de saída.
A partir disso, a camada consegue reduzir o tamanho da matriz de entrada, fazendo com que o poder computacional
necessário seja reduzido, junto com o uso de memória.
(TODO: Adicionar figuras, referências e detalhar mais)

\subsection{Camada totalmente conectada}\label{cap_conceitos_cnn_totalmente}
% ---
A camada totalmente conectada (\textit{fully connected layer}) é a camada final de uma CNN.
Depois das camadas anteriores extraírem as características da imagem, a camada totalmente
conectada as interpreta e gerar uma resposta.
(TODO: Revisar texto)

\section{\textit{Data augmentation}}
% ---
CNNs tem um ótimo desempenho em tarefas de visão computacional. Entretanto esse tipo de rede neural precisa de uma
grande quantidade dados para não sofrer de \textit{overfitting} (superajuste). \cite{shorten2019survey}
Esse é o objetivo do \textit{data augmentation} (aumento de dados), gerar mais dados a partir de um conjunto de dados
que já existe, aplicando algumas transformações geométricas ou espaciais, ou realizam injeção de ruído nas imagens
originais.

% TODO: Adicionar exemplo de data augmentation.

\section{Transferência de conhecimento}\label{conceitos_transferencia}
% ---
Transferência de conhecimento consiste em usar um modelo pré-treinado em uma base de dados específica e aproveitar
o conhecimento adquirido durante esse treinamento para um novo conjunto de dados.
É necessário que o problema do \textit{dataset} (conjunto de dados) atual seja um subconjunto do \textit{dataset}
que foi usado para treinar o modelo base.

Para realizar a transferência de conhecimento é necessário adaptar a camada de entrada e de saída
(totalmente conectada) do modelo base, para que ocorra um pré-processamento dos dados de entrada
(antes deles serem passados para o modelo base), além disso é necessário definir e treinar a camada totalmente
conectada com o \textit{dataset} do problema.

\section{Métodos de compressão para Redes Neurais}
ANNs são utilizadas em várias aplicações, demonstrando habilidades extraordinárias no campo de visão computacional.
No entanto, redes com arquiteturas complexas são um desafio para a implantação em tempo real e necessitam de uma
grande quantidade de energia e poder computacional \cite{LIANG2021370}.
Por causa disso foram desenvolvidos métodos para reduzir o tamanho dessas redes, as tornando mais eficiente.
Nesse trabalho os métodos de poda (\ref{poda}), quantização (\ref{quantizacao}) e destilamento do conhecimento
(\ref{conceitos_destilamento}) serão usados.

\subsection{\textit{Pruning}(Poda)}\label{poda}
A poda de redes neurais tem como objetivo principal remover pesos ou neurônios que são
redundantes ou irrelevantes para o problema, além disso reduz o \textit{overfitting}(superajuste).
(TODO: escrever sobre poda de redes neurais)

% Redes neurais costumam usar muito recurso computacional e otimizar o uso dos recursos é muito importante para
% aplicações de grande escala. A poda de redes neurais tem como objetivo principal remover pesos ou neurônios que são
% redundantes ou irrelevantes para o problema, além disso reduz o \textit{overfitting}(superajuste).

\subsection{Quantização}\label{quantizacao}

% Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations
% may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including
% binary neural networks. Both pruning and quantization can be used independently or combined.

Quantização reduz a computação diminuindo a precisão dos tipos de dados. Pesos, \textit{bias} (vieses) e ativações
geralmente devem ser quantizadas para inteiros de 8 bit, embora implementações menores que 8 bit sejam discutidas
incluindo redes neurais binárias. \cite{LIANG2021370}

% (TODO: Escrever sobre quantização)

\subsection{Destilamento de conhecimento (Professor-Aluno)}\label{conceitos_destilamento}

Destilamento de conhecimento ou \textit{knowledge distillation} \cite{hinton2015distilling}, é uma técnica que tem
como objetivo treinar um modelo Aluno (menor e sem pré-treinamento) com um modelo Professor
(maior e com pré-treinamento). Ela é amplamente utilizada para as áreas de visão computacional e linguagem natural,
e tem como objetivo reduzir o tamanho do modelo final (Aluno).

Para transferir o conhecimento do modelo Professor para o Aluno, a técnica utiliza os \textit{logits} (entrada da
função de ativação final \textit{softmax}) no lugar da classe prevista. Além disso, são utilizado os
\textit{soft targets} (probabilidades das classes previstas pelo modelo Professor) junto com os
\textit{hard targets} (classe esperada). Então, o Aluno é treinado com uma porcentagem $\alpha$ do erro com o
\textit{hard target} e $\alpha - 1$ do erro com \textit{soft target}, assim é calculado o erro do aluno.

% NOTE: Talvez detalhar a temperatura?

% Destilamento de conhecimento ou \textit{knowledge distillation} \cite{hinton2015distilling}, é uma técnica de
% treinamento que utiliza dois modelos, Professor que é um modelo mais robusto e pré-treinado para o problema, e
% o modelo Aluno que é o modelo que (geralmente) é mais leve que o professor e não possui nenhum pré-treinamento.
% Para destilar o conhecimento do professor para o estudante, utilizamos os \textit{logits}, que são os valores de
% entrada da camada \textit{softmax}, então os \textit{logits} do \emph{professor} são comparados com os do
% \emph{estudante}.
% TODO: escrever sobre destilamento de conhecimento

\section{Otimização Bayesiana}
% ---
Testar diferentes valores para os hiperparâmetros é uma tarefa essencial para otimizar o desempenho de ANNs.
A otimização Bayesiana é um dos métodos utilizados para fazer esse teste, ela possui dois componentes principais,
o modelo estatístico Bayesiano, que modelar a função objetiva, e a função de aquisição, que decide a próxima amostra
de parâmetros. % TODO: Adicionar citação

% TODO: Escrever sobre otimização Bayesiana.
