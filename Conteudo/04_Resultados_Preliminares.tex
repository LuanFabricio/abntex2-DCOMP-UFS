\chapter{Resultados preliminares}

Neste capítulo serão apresentados testes feitos antes do projeto, eles tiveram a finalidade de exercitar o
conteúdo estudado durante o trabalho. O objetivo principal é usar as técnicas de compressão de modelos para
criar modelos menores e mais eficientes.

% TODO: Revisar

% Destilamento de conhecimento
\section{Destilamento de conhecimento (modelo Professor-Aluno)}
Para fazer o experimento com destilamento de conhecimento foi utilizada da base STL-10, que possui 500 imagens para
treinamento e 800 para teste, com resolução de $96 \times 96$ e 3 canais de cor (RGB). Como o conjunto de dados
não possui muitas imagens, foi aplicada a técnica de \textit{data augmentation} (aumento de dados) para reduzir o
\textit{overfitting}.

Como já foi descrito na \autoref{conceitos_destilamento}, o objetivo dessa etapa é utilizar o conhecimento do modelo
Professor(mais robusto e pré-treinado) para treinar o modelo Aluno (mais simples e sem pré-treinamento).
Onde o modelo professor (\autoref{res_professor}) é a ResNet-50  \cite{resnet} e o modelo estudante é gerado pelo
\autoref{res_aluno_1}.
Além disso o modelo Rafael \cite{rafael} foi adaptado e utilizado (com algumas variações).

Para aumentar a precisão do modelo Aluno com o destilamento de conhecimento, foi utilizada a otimização
Bayesiana, para procurar os valores dos hiperparâmetros $\alpha$ e \textit{Temperature}.
Os possíveis valores de $\alpha$ foram 0.1, 0.5, 0.01 e 0.25.
E os possíveis valores de \textit{Temperature} foram 2, 5, 7, 10, 12, 15, 17 e 20.
Os resultados do experimento estão na \autoref{tabela_acuracia_1}.

\begin{center}
\begin{table}[htb]
\centering
\ABNTEXfontereduzida
\caption[Acurácia dos modelos]{Acurácia dos modelos.}
\label{tabela_acuracia_1}
\begin{tabular}{ |c|c|c|c|c| }
	\hline
	\textbf{Modelo} & \textbf{Com destilamento de conhecimento?}  & \textbf{Acurácia (validação)}
		   & \textbf{$\alpha$} & \textbf{\textit{Temperature}} \\
	\hline
	ResNet-50 	& 	Não 	& 	90,65\%	& 	- 	& 	-	 \\
	Aluno 		& 	Não 	& 	76,80\%	& 	- 	& 	-	 \\
	Rafael-1 	& 	Não 	& 	67,08\%	& 	- 	& 	-	 \\
	Rafael-base 	& 	Não 	& 	71,38\%	& 	- 	& 	-	 \\
	Rafael-2 	& 	Não 	& 	74,57\%	& 	- 	& 	-	 \\
	Rafael-3 	& 	Não 	& 	71,01\%	& 	- 	& 	-	 \\
	Aluno 		& 	Sim 	& 	83,79\%	& 	0,1 	& 	5	 \\
	Rafael-base 	& 	Sim 	& 	74,21\%	& 	0,1 	& 	10	 \\
	Rafael-1 	& 	Sim 	& 	69,70\%	& 	0,01 	& 	20	 \\
	Rafael-2 	& 	Sim 	& 	79,12\%	& 	0,01 	& 	7	 \\
	Rafael-3 	& 	Sim 	& 	76,55\%	& 	0,01 	& 	5	 \\
	\hline
\end{tabular}
\legend{Fonte: Autor}
\end{table}
\end{center}

Na \autoref{tabela_acuracia_1}, a variação Rafael-1 indica que foi adicionada uma camada $9x9$ no modelo, Rafael-2
indica que foi adicionada duas camadas $3x3$ e Rafael-3 indica que foi adicionada três camadas $3x3$.

\section{\textit{Pruning} e Quantização}

Para esse experimento foi utilizada a base CIFAR-10, que possui 6.000 imagens para cada classe, sendo que essas imagens
tem resolução igual $32x32$ e possui 3 canais de cores (RGB). Nesse conjunto de dados também foi necessário fazer
\textit{data augmentation} para aumentar a precisão do modelo final.

O modelo utilizado no experimento foi gerado pelo \autoref{pruning_quantization_model}, inicialmente ele possui
2.397.226 parâmetros (pesando 28 MB). Inicialmente ele é treinado durante 25 \textit{epochs} (épocas), atingindo
uma acurácia de $90,18\%$ nos dados de treinamento e $85,91\%$ nos dados de validação.

Depois de treinado, o modelo é podado utilizando a estratégia de \textit{prune low magnitude} (podar baixa magnitude),
que tem como foco zerar valores abaixo de um certo limiar. Depois de definir os parâmetros da poda, o modelo é
retreinado por 2 \textit{epochs}, para que o algoritmo de poda consiga identificar as conexões importantes durante o
uso do modelo. Após o retreinamento, o modelo final tem acurácia de $83,83\%$ nos dados de treinamento e $84,40\%$
nos dados de validação, pesando 9,3 MB após remover todos os valores iguais a zero.

Depois de podado, modelo foi convertido para TensorFlow Lite, consumindo 9,2 MB de armazenamento e ficando com
$84,91\%$ de precisão. Depois disso, a quantização é aplicada utilizando a API do TensorFlow Lite, deixando o modelo
final com 2,4 MB e $84,36\%$ de precisão.

% TODO: Fazer etapa de quantização.

\begin{center}
\begin{table}[htb]
\centering
\ABNTEXfontereduzida
\caption[Acurácia e peso dos modelos]{Acurácia e peso dos modelos.}
\label{tabela_acuracia_peso}
\begin{tabular}{ |c|c|c| }
	\hline
	\textbf{Modelo} & \textbf{Acurácia (validação)}  & \textbf{\textit{Footprint} (MB)} \\
	\hline
	Modelo base 				 & 	85,91\% 	& 	28	\\
	Modelo base podado 			 & 	84,40\% 	& 	9,3	\\
	Modelo base podado (TFLite) 		 & 	84,40\% 	& 	9,2	\\
	Modelo base podado e quantizado (TFLite) & 	84,36\% 	& 	2,4	\\
	\hline
\end{tabular}
\legend{Fonte: Autor}
\end{table}
\end{center}
